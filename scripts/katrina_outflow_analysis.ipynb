{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Katrina Outflow Analysis\n",
    "## Organising the Dataset\n",
    "We import ```pandas```, ```numpy```, ```csv```, and ```os``` libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also import the ```fill_table``` function which we will use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fill_table import fill_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the path to the ```IRS_migration_data``` repository folder in a string variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = os.getcwd()[0:len(os.getcwd())-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a new ```tables``` folder in which we will store the tables produced by this script, it will be a subfolder of your ```IRS_migration_data``` repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = repo_path + 'tables/'\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload the data on outflows from a csv file.\n",
    "It covers the period 1998-2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outflow_df = pd.read_csv(repo_path + 'outflows/outflow.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the first 10 lines of the dataframe wew just created to see how it is structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outflow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store in a numpy array the unique fip codes of destination and origin counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = outflow_df[(outflow_df['state_code_origin']<=56) & ((outflow_df['state_code_dest']<=56))]\n",
    "origin_codes = pd.unique(origins['origin'].values)\n",
    "destination_codes = np.append(origin_codes, [58000,59000,57009])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload the cleaned and restructured datasets for the nine period we will analyse:\n",
    "    \n",
    "* before Katrina (1999-2004);\n",
    "* and after Katrina (2007-2009)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_1 = pd.read_csv(repo_path + 'outflows/9900out.csv')\n",
    "pre_1.rename(columns={'Unnamed: 0':''}, inplace=True)\n",
    "pre_1.set_index([''], inplace=True)\n",
    "new_col_names = list(map(int, pre_1.columns.values))\n",
    "pre_1.columns = new_col_names\n",
    "\n",
    "pre_2 = pd.read_csv(repo_path + 'outflows/0001out.csv')\n",
    "pre_2.rename(columns={'Unnamed: 0':''}, inplace=True)\n",
    "pre_2.set_index([''], inplace=True)\n",
    "new_col_names = list(map(int, pre_2.columns.values))\n",
    "pre_2.columns = new_col_names\n",
    "\n",
    "pre_3 = pd.read_csv(repo_path + 'outflows/0102out.csv')\n",
    "pre_3.rename(columns={'Unnamed: 0':''}, inplace=True)\n",
    "pre_3.set_index([''], inplace=True)\n",
    "new_col_names = list(map(int, pre_3.columns.values))\n",
    "pre_3.columns = new_col_names\n",
    "\n",
    "pre_4 = pd.read_csv(repo_path + 'outflows/0203out.csv')\n",
    "pre_4.rename(columns={'Unnamed: 0':''}, inplace=True)\n",
    "pre_4.set_index([''], inplace=True)\n",
    "new_col_names = list(map(int, pre_4.columns.values))\n",
    "pre_4.columns = new_col_names\n",
    "\n",
    "pre_5 = pd.read_csv(repo_path + 'outflows/0304out.csv')\n",
    "pre_5.rename(columns={'Unnamed: 0':''}, inplace=True)\n",
    "pre_5.set_index([''], inplace=True)\n",
    "new_col_names = list(map(int, pre_5.columns.values))\n",
    "pre_5.columns = new_col_names\n",
    "\n",
    "pre_6 = pd.read_csv(repo_path + 'outflows/0405out.csv')\n",
    "pre_6.rename(columns={'Unnamed: 0':''}, inplace=True)\n",
    "pre_6.set_index([''], inplace=True)\n",
    "new_col_names = list(map(int, pre_6.columns.values))\n",
    "pre_6.columns = new_col_names\n",
    "\n",
    "re_1 = pd.read_csv(repo_path + 'outflows/0708out.csv')\n",
    "re_1.rename(columns={'Unnamed: 0':''}, inplace=True)\n",
    "re_1.set_index([''], inplace=True)\n",
    "new_col_names = list(map(int, re_1.columns.values))\n",
    "re_1.columns = new_col_names\n",
    "\n",
    "re_2 = pd.read_csv(repo_path + 'outflows/0809out.csv')\n",
    "re_2.rename(columns={'Unnamed: 0':''}, inplace=True)\n",
    "re_2.set_index([''], inplace=True)\n",
    "new_col_names = list(map(int, re_2.columns.values))\n",
    "re_2.columns = new_col_names\n",
    "\n",
    "re_3 = pd.read_csv(repo_path + 'outflows/0910out.csv')\n",
    "re_3.rename(columns={'Unnamed: 0':''}, inplace=True)\n",
    "re_3.set_index([''], inplace=True)\n",
    "new_col_names = list(map(int, re_3.columns.values))\n",
    "re_3.columns = new_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import a set of csv files that contain the fip codes for different groups of counties we will use in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_counties_df = pd.read_csv(repo_path + 'county_groups/disaster_kat_counties.csv', usecols = ['fip_code'])\n",
    "nearby_counties_df = pd.read_csv(repo_path + 'county_groups/nearby_kat_counties.csv', usecols = ['fip_code'])\n",
    "distant_counties_df = pd.read_csv(repo_path + 'county_groups/distant_kat_counties.csv', usecols = ['fip_code'])\n",
    "urban_nc_counties_df = pd.read_csv(repo_path + 'county_groups/urban_nc_counties.csv', usecols = ['fip_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert the dataframes into lists and we add one list with all the counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_counties = list(disaster_counties_df['fip_code'])\n",
    "nearby_counties = list(nearby_counties_df['fip_code'])\n",
    "distant_counties = list(distant_counties_df['fip_code'])\n",
    "urban_nc_counties = list(urban_nc_counties_df['fip_code'])\n",
    "all_nc_counties = disaster_counties + nearby_counties + distant_counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using list comprehension, we divide all the groups we have defined so far into urban and rural areas by looking at their 2010 Census population. If the proportion living in rural areas is equal or above 50% we classify the county as rural otherwise as urban."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_urban_counties = [x for x in disaster_counties if x in urban_nc_counties]\n",
    "nearby_urban_counties = [x for x in nearby_counties if x in urban_nc_counties]\n",
    "distant_urban_counties = [x for x in distant_counties if x in urban_nc_counties]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now summarize the number of counties in each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', len(disaster_counties), 'disaster counties, of which', \n",
    "      len(disaster_urban_counties), 'are urban.' )\n",
    "\n",
    "print('There are', len(nearby_counties), 'nearby counties, of which', \n",
    "      len(nearby_urban_counties), 'are urban.' )\n",
    "\n",
    "print('There are', len(distant_counties), 'distant counties, of which', \n",
    "      len(distant_urban_counties), 'are urban.' )\n",
    "\n",
    "print('There is a total of', len(all_nc_counties), 'counties, of which', \n",
    "      len(urban_nc_counties), 'are urban and the remaining',\n",
    "      len(all_nc_counties) - len(urban_nc_counties), 'are rural.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later use, we create a list containing al the lists of counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [all_nc_counties, \n",
    "              disaster_counties, \n",
    "              nearby_counties, \n",
    "              distant_counties, \n",
    "              urban_nc_counties, \n",
    "              disaster_urban_counties, \n",
    "              nearby_urban_counties, \n",
    "              distant_urban_counties]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ties Analysis\n",
    "We create six dataframes, one for each year considered, where we have the ties connecting each county to the others. Here a tie is defined as the presence of a flow of any size between two counties. The final result is a matrix for each period whose rows and columns are all the counties in the dataset and where a 1 indicates the presence of a tie between the two counties and a 0 its absence. We consider a tie to exist if a positive flow was recorded at least in one of the years composing the before and after periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_1_ties = pre_1.drop([58000,59000,57009],axis=1).where(pre_1==0,1)\n",
    "pre_2_ties = pre_2.drop([58000,59000,57009],axis=1).where(pre_2==0,1)\n",
    "pre_3_ties = pre_3.drop([58000,59000,57009],axis=1).where(pre_3==0,1)\n",
    "pre_4_ties = pre_4.drop([58000,59000,57009],axis=1).where(pre_4==0,1)\n",
    "pre_5_ties = pre_5.drop([58000,59000,57009],axis=1).where(pre_5==0,1)\n",
    "pre_6_ties = pre_6.drop([58000,59000,57009],axis=1).where(pre_6==0,1)\n",
    "\n",
    "re_1_ties = re_1.drop([58000,59000,57009],axis=1).where(re_1==0,1)\n",
    "re_2_ties = re_2.drop([58000,59000,57009],axis=1).where(re_2==0,1)\n",
    "re_3_ties = re_3.drop([58000,59000,57009],axis=1).where(re_3==0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ties = (pre_1_ties + pre_2_ties + pre_3_ties + pre_4_ties + pre_5_ties + pre_6_ties)\n",
    "pre_ties = pre_ties.where(pre_ties==0,1)\n",
    "\n",
    "re_ties = (re_1_ties + re_2_ties + re_3_ties)\n",
    "re_ties = re_ties.where(re_ties==0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result is a matrix for each period whose rows and columns are all the counties in the dataset and where a 1 indicates the presence of a tie between the two counties and a 0 its absence. We consider a tie to exist if a positive flow was recorded at least in one of the three years composing the before and after periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ties = re_ties - pre_ties\n",
    "uties_pre = ties.where(ties==-1,0)*-1\n",
    "uties_re = ties.where(ties==1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in uties_pre.index:\n",
    "    uties_pre.loc[i, i] = 0\n",
    "for i in uties_re.index:\n",
    "    uties_re.loc[i, i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the table headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_groups = ['All','Disaster Affected','Nearby','Distant',\n",
    "                   'All (Urban)', 'Disaster Affected (Urban)', 'Nearby (Urban)','Distant (Urban)']\n",
    "periods = ['Pre-Disaster','Recovery']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the ```fill_table``` function that we will use to fill and then print the tables summarising ties and flows from different county groups across the two periods. You can write ```print(fill_table.__doc__)``` to know more about the function. In particular, you can pass ```False``` to the ```change_col``` argument if you don't want the change column and ```True``` to the ```print_table``` argument if you want to print the table directly from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ties_df = fill_table([uties_pre,uties_re], group_list, \n",
    "           disaster_counties, counties_groups, periods, print_table = False)\n",
    "ties_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the table to a csv file, uncomment the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ties_df.to_csv(results_path + 'outties_table_katrina.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flows Analysis\n",
    "We create to dataframes with the same structure as the ones with the ties but containing average flows for the two periods respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_avg = (pre_1 + pre_2 + pre_3 + pre_4 + pre_5 + pre_6)/6\n",
    "re_avg = (re_1 + re_2 + re_3)/3\n",
    "\n",
    "pre_avg = pre_avg.round(decimals=0)\n",
    "re_avg = re_avg.round(decimals=0)\n",
    "\n",
    "pre_flows = pre_avg\n",
    "re_flows = re_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove flows in the main diagonal as these represent household that remained in the same counties and are thus not interesting in our migration analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pre_flows.index:\n",
    "    pre_flows.loc[i, i] = 0\n",
    "for i in re_flows.index:\n",
    "    re_flows.loc[i, i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill and print the flow table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_df = fill_table([pre_avg,re_avg], group_list, \n",
    "           disaster_counties, counties_groups, periods, print_table = False)\n",
    "flows_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the table to a csv file, uncomment the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flows_df.to_csv(results_path + 'outflows_table_katrina.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
